# <!-- Powered by BMAD™ Core -->

# Story 2.2: Integracja LLM (OpenAI, non‑stream) w BFF

## Status

Approved

## Story

**As a** user,
**I want** the application to use a real Large Language Model (LLM) to generate the artist's description,
**so that** the output is high-quality and based on the provided context.

## Acceptance Criteria

1.  A real description generated by an LLM is returned in the `text` field of the API response.
2.  The `modelName` and `tokensUsed` fields are correctly populated in the API response.
3.  There are no breaking changes to the input/output contract of the `/api/audio/generate` endpoint.
4.  The `.env.example` and `README.md` files are updated with the new environment variables `LLM_API_KEY` and `LLM_MODEL`.

## Dev Notes

### Previous Story Insights

- **Architectural Context**: Story 2.1 established an asynchronous flow where the front-end calls `/api/audio/analyze` to start the analysis and polls `/api/audio/analyze/status` for the result. The `/api/audio/generate` endpoint now receives the `analysisId` to retrieve the completed analysis data. This pattern MUST be maintained. The LLM integration should be the final step in the `generate` endpoint's orchestration. [Source: Story 2.1 `Dev Agent Record`]

### API Specifications

- **LLM Invocation**: The BFF is solely responsible for calling the LLM provider. The client-side MUST NOT have any direct interaction with the LLM. [Source: docs/architecture/7-external-apis.md#72-llm--text-generation-provider-tbc]
- **Interaction Mode**: The interaction with the LLM provider must be a standard JSON (non-stream) request/response. [Source: docs/architecture/5-api-specification.md#51-post-api-audio-generate]
- **Prompt Formulation**: The prompt sent to the LLM should be constructed within the BFF, combining the `artistName`, `artistDescription`, and the `AudioAnalysis` data retrieved from the completed analysis job. [Source: docs/architecture/7-external-apis.md#72-llm--text-generation-provider-tbc]
- **Error Handling**: Handle potential 429 (rate limiting) and 5xx (server errors) from the LLM provider. These should be mapped to the standard `ApiError` format and include a `requestId`. A simple retry mechanism (max 2 retries with backoff) is recommended. [Source: docs/architecture/7-external-apis.md#72-llm--text-generation-provider-tbc]

### Data Models

- **Output Model (`GeneratedDescription`)**: The successful response from `/api/audio/generate` must conform to this structure: `{ language: 'pl'|'en', text: string, outline?: string[], modelName?: string, tokensUsed?: number }`. [Source: docs/architecture/data-models.md#43-modele-wyjciowe-be--fe]
- **Error Model (`ApiError`)**: All errors must conform to the standard structure: `{ error: { code: string, message: string, details?: Record<string, any>, timestamp: string, requestId: string } }`. [Source: docs/architecture/data-models.md#43-modele-wyjciowe-be--fe]

### File Locations

- **New Service Logic**: All new logic for interacting with the LLM provider should be encapsulated in a new file at `src/lib/server/llm.ts`. [Source: docs/architecture/11-backend-architecture-opcjonalny-bff.md#111-struktura-katalogow]
- **Orchestration**: The new `llm.ts` service will be called from the existing `src/app/api/audio/generate/route.ts` handler. [Source: docs/architecture/11-backend-architecture-opcjonalny-bff.md#111-struktura-katalogow]

### Technical Constraints

- **Environment Variables**: The implementation must use `LLM_API_KEY` for the provider's API key and `LLM_MODEL` to specify the model name (e.g., `gpt-4o-mini`). These must not be exposed to the client. [Source: docs/architecture/7-external-apis.md#73-zmienne-rodowiskowe-env]

## Tasks / Subtasks

- [ ] **Task 1: Create LLM Service** (AC: 1, 2)

  - [ ] Create the file `src/lib/server/llm.ts`.
  - [ ] Implement a function (e.g., `generateDescription`) that accepts `artistName`, `artistDescription`, and `AudioAnalysis` to build a prompt.
  - [ ] Use an HTTP client (e.g., `fetch`) to call the LLM provider's API using the `LLM_API_KEY` and `LLM_MODEL` from environment variables.
  - [ ] Parse the response and return an object matching the `GeneratedDescription` interface, including `text`, `modelName`, and `tokensUsed`.
  - [ ] Implement error handling to catch and re-throw errors in a standardized way.

- [ ] **Task 2: Integrate LLM Service into Generate API** (AC: 1, 2, 3)

  - [ ] In `src/app/api/audio/generate/route.ts`, after successfully retrieving the audio analysis results, call the new `generateDescription` function from `llm.ts`.
  - [ ] Pass the required inputs (`artistName`, `artistDescription`, and the analysis data) to the service.
  - [ ] Map any errors from the LLM service to the standard `ApiError` response format.
  - [ ] Return the successful `GeneratedDescription` object to the client with a 200 status.

- [ ] **Task 3: Update Documentation** (AC: 4)

  - [ ] Add `LLM_API_KEY` and `LLM_MODEL` to the `.env.example` file with placeholder values.
  - [ ] Update `README.md` to explain what these new environment variables are for and how to obtain them.

- [ ] **Task 4: Implement Testing** (AC: 1, 2, 3)
  - [ ] Create unit tests for `src/lib/server/llm.ts`. Mock the `fetch` call to the LLM provider to test both success and failure scenarios.
  - [ ] Update the integration tests for the `/api/audio/generate` endpoint to verify that a mocked LLM response is correctly returned. Ensure the end-to-end flow remains intact.

## Change Log

| Date       | Version | Description                 | Author             |
| ---------- | ------- | --------------------------- | ------------------ |
| 2025-08-31 | 1.0     | Initial draft for Story 2.2 | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
