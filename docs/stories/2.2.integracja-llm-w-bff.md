# <!-- Powered by BMAD™ Core -->

# Story 2.2: Integracja LLM (OpenAI, non‑stream) w BFF

## Status

Ready for Review

## Story

**As a** user,
**I want** the application to use a real Large Language Model (LLM) to generate the artist's description,
**so that** the output is high-quality and based on the provided context.

## Acceptance Criteria

1.  A real description generated by an LLM is returned in the `text` field of the API response.
2.  The `modelName` and `tokensUsed` fields are correctly populated in the API response.
3.  There are no breaking changes to the input/output contract of the `/api/audio/generate` endpoint.
4.  The `.env.example` and `README.md` files are updated with the new environment variables `LLM_API_KEY` and `LLM_MODEL`.

## Dev Notes

### Previous Story Insights

- **Architectural Context**: Story 2.1 established an asynchronous flow where the front-end calls `/api/audio/analyze` to start the analysis and polls `/api/audio/analyze/status` for the result. The `/api/audio/generate` endpoint now receives the `analysisId` to retrieve the completed analysis data. This pattern MUST be maintained. The LLM integration should be the final step in the `generate` endpoint's orchestration. [Source: Story 2.1 `Dev Agent Record`]

### API Specifications

- **LLM Invocation**: The BFF is solely responsible for calling the LLM provider. The client-side MUST NOT have any direct interaction with the LLM. [Source: docs/architecture/7-external-apis.md#72-llm--text-generation-provider-tbc]
- **Interaction Mode**: The interaction with the LLM provider must be a standard JSON (non-stream) request/response. [Source: docs/architecture/5-api-specification.md#51-post-api-audio-generate]
- **Prompt Formulation**: The prompt sent to the LLM should be constructed within the BFF, combining the `artistName`, `artistDescription`, and the `AudioAnalysis` data retrieved from the completed analysis job. [Source: docs/architecture/7-external-apis.md#72-llm--text-generation-provider-tbc]
- **Error Handling**: Handle potential 429 (rate limiting) and 5xx (server errors) from the LLM provider. These should be mapped to the standard `ApiError` format and include a `requestId`. A simple retry mechanism (max 2 retries with backoff) is recommended. [Source: docs/architecture/7-external-apis.md#72-llm--text-generation-provider-tbc]

### Data Models

- **Output Model (`GeneratedDescription`)**: The successful response from `/api/audio/generate` must conform to this structure: `{ language: 'pl'|'en', text: string, outline?: string[], modelName?: string, tokensUsed?: number }`. [Source: docs/architecture/data-models.md#43-modele-wyjciowe-be--fe]
- **Error Model (`ApiError`)**: All errors must conform to the standard structure: `{ error: { code: string, message: string, details?: Record<string, any>, timestamp: string, requestId: string } }`. [Source: docs/architecture/data-models.md#43-modele-wyjciowe-be--fe]

### File Locations

- **New Service Logic**: All new logic for interacting with the LLM provider should be encapsulated in a new file at `src/lib/server/llm.ts`. [Source: docs/architecture/11-backend-architecture-opcjonalny-bff.md#111-struktura-katalogow]
- **Orchestration**: The new `llm.ts` service will be called from the existing `src/app/api/audio/generate/route.ts` handler. [Source: docs/architecture/11-backend-architecture-opcjonalny-bff.md#111-struktura-katalogow]

### Technical Constraints

- **Environment Variables**: The implementation must use `LLM_API_KEY` for the provider's API key and `LLM_MODEL` to specify the model name (e.g., `gpt-4o-mini`). These must not be exposed to the client. [Source: docs/architecture/7-external-apis.md#73-zmienne-rodowiskowe-env]

## Tasks / Subtasks

- [x] **Task 1: Create LLM Service** (AC: 1, 2)

  - [x] Create the file `src/lib/server/llm.ts`.
  - [x] Implement a function (e.g., `generateDescription`) that accepts `artistName`, `artistDescription`, and `AudioAnalysis` to build a prompt.
  - [x] Use an HTTP client (e.g., `fetch`) to call the LLM provider's API using the `LLM_API_KEY` and `LLM_MODEL` from environment variables.
  - [x] Parse the response and return an object matching the `GeneratedDescription` interface, including `text`, `modelName`, and `tokensUsed`.
  - [x] Implement error handling to catch and re-throw errors in a standardized way.

- [x] **Task 2: Integrate LLM Service into Generate API** (AC: 1, 2, 3)

  - [x] In `src/app/api/audio/generate/route.ts`, after successfully retrieving the audio analysis results, call the new `generateDescription` function from `llm.ts`.
  - [x] Pass the required inputs (`artistName`, `artistDescription`, and the analysis data) to the service.
  - [x] Map any errors from the LLM service to the standard `ApiError` response format.
  - [x] Return the successful `GeneratedDescription` object to the client with a 200 status.

- [x] **Task 3: Update Documentation** (AC: 4)

  - [x] Add `LLM_API_KEY` and `LLM_MODEL` to the `.env.example` file with placeholder values.
  - [x] Update `README.md` to explain what these new environment variables are for and how to obtain them.

- [x] **Task 4: Implement Testing** (AC: 1, 2, 3)
  - [x] Create unit tests for `src/lib/server/llm.ts`. Mock the `fetch` call to the LLM provider to test both success and failure scenarios.
  - [x] Update the integration tests for the `/api/audio/generate` endpoint to verify that a mocked LLM response is correctly returned. Ensure the end-to-end flow remains intact.

## Change Log

| Date       | Version | Description                 | Author             |
| ---------- | ------- | --------------------------- | ------------------ |
| 2025-08-31 | 1.0     | Initial draft for Story 2.2 | Bob (Scrum Master) |
| 2025-09-01 | 1.1     | QA fixes: add 30s LLM timeout, env assertions for LLM vars, tests updated | James (Dev) |

## Dev Agent Record

### Agent Model Used

- Cascade (James / Dev), 2025-09-01

### Debug Log References

- `src/lib/server/llm.ts` — non-stream LLM integration, prompt builder, retries (429/5xx), env guard via `getServerEnv()`.
- `src/app/api/audio/generate/route.ts` — integration of `llm.generateDescription`, error mapping to `ApiError`, outline retained for UX.
- `.env.example` — added `LLM_API_KEY`, `LLM_MODEL`, optional `LLM_SYSTEM_PROMPT`.
- `README.md` — documented LLM env vars and behavior.
- `src/lib/server/__tests__/llm.test.ts` — unit tests (success, retry, 5xx failure).
- `src/lib/api/__tests__/generate.test.ts` — client-side integration test for `/api/audio/generate` flow.
- QA Fixes: enforced 30s timeout in `src/lib/server/llm.ts` using `AbortController`; extended `assertServerEnv()` in `src/lib/server/env.ts` to validate `LLM_API_KEY` and `LLM_MODEL`; added unit tests for timeout and missing env.

### Completion Notes List

- Implemented `generateDescription` using OpenAI Chat Completions (JSON, non-stream) with simple backoff retries.
- Prompt composed from `artistName`, `artistDescription`, and `AnalyzedTrack` signals; returns `text`, `modelName`, `tokensUsed`.
- API route `/api/audio/generate` calls LLM service and returns successful payload; errors mapped to standard `ApiError`.
- Updated `.env.example` and `README.md` with `LLM_API_KEY`, `LLM_MODEL`, optional `LLM_SYSTEM_PROMPT`.
- Added unit tests for LLM service; client integration test covers generate flow.
- QA follow-ups: add explicit request timeout (~30s) in `llm.ts`; env presence check already included, consider startup validation.
- Applied QA fixes: added 30s timeout guard to LLM fetch; standardized missing env handling to `ENV_MISSING` with details; extended server env assertion to include LLM vars; added tests covering timeout and env-missing cases.

### File List

- Added: `src/lib/server/llm.ts`
- Modified: `src/app/api/audio/generate/route.ts`
- Updated: `.env.example`, `README.md`
- Added tests: `src/lib/server/__tests__/llm.test.ts`
- Existing tests used: `src/lib/api/__tests__/generate.test.ts`
- Modified: `src/lib/server/env.ts`, `src/lib/server/__tests__/llm.test.ts`

## QA Results

**Gate Decision:** PASS

**Review Date:** 2025-09-01
**Reviewer:** Quinn (Test Architect)

**Summary:**
The implementation is robust and addresses all initial concerns. The developer has successfully integrated a 30-second timeout for the LLM API call and implemented comprehensive checks for required environment variables, both at the service level and on application startup. The accompanying tests verify this new functionality. The story meets all quality criteria.

**Concerns:**
- All previous concerns have been addressed and verified.

**Next Steps:**
The story is approved for deployment.
